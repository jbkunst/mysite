[
{
    "pk": 2, 
    "model": "blog.entry", 
    "fields": {
        "body": "<p>This post join two things that I like: Highcharts and the Time series. So I have a lot of fun making this post to show how works an autoregressive process. Well, let's remember the structure of an autoregresive process.  $ y_t = \\varphi\\,y_{t-1}+\\epsilon_t $, where $\\epsilon_t$ is a white noise, i.e, a $cov(\\epsilon_j, \\epsilon_i) = 0$ if $i \\neq j$, $Var(\\epsilon_t) = \\sigma^2_\\epsilon$, and $\\varphi$ is a parameter.</p>\r\n<p>Now, let's see first how a white noise looks like. The next chart show the process $y_t = \\epsilon_t$, so if we see for a moment the chart we'll notice the process look likes an independent random sequence. There's no structure of any dependence.</p>\r\n<script src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js\" type=\"text/javascript\"></script>\r\n<script src=\"../../../../media/highcharts_arma_js/ar_model_0.js\" type=\"text/javascript\"></script>\r\n<script src=\"../../../../media/highcharts_arma_js/ar_model_1.js\" type=\"text/javascript\"></script>\r\n<script src=\"../../../../media/highcharts_arma_js/ar_model_2.js\" type=\"text/javascript\"></script>\r\n<script src=\"../../../../media/js/highcharts/highcharts.js\"></script>\r\n<div id=\"container_m0\" style=\"width: 600px; height: 200px; text-align: center;\">&nbsp;</div>\r\n<p>The two charts above show autoregressive process for two distinct values of $\\varphi$. It's easy to see the difference. In the first process the next value of the serie tends to have the same sign of the past value. In the other procces ocurrs the oposite. That's the effect of the sign of the parameter.</p>\r\n<div id=\"container_m1\" style=\"width: 600px; height: 200px; text-align: center;\">&nbsp;</div>\r\n<div id=\"container_m2\" style=\"width: 600px; height: 200px; text-align: center;\">&nbsp;</div>\r\n<p>It's interesting to note: the two process are diferents but they have the same mean and variance. In fact, in the $AR(1)$ process $Var(y_t) = \\frac{\\sigma_\\epsilon^2}{1-\\varphi^2}$, only if the process are stationary ($|\\varphi|&lt;1$).</p>", 
        "status": 1, 
        "allow_comments": false, 
        "title": "Live Autoregressive Process", 
        "image": "images_pf/ts_1.gif", 
        "category": [
            4, 
            3
        ], 
        "pub_date": "2012-08-27T17:38:02Z", 
        "slug": "live-autoregressive-process"
    }
},
{
    "pk": 3, 
    "model": "blog.entry", 
    "fields": {
        "body": "<p>I searched for a javascript plugin to highlighter code,&nbsp;but obviously not any code. I needed to higlight R. Finally I founded&nbsp;<a href=\"http://softwaremaniacs.org/soft/highlight/en/\" target=\"_blank\">highlight.js</a>. Highlights.js is pretty cool for many reasons. I like&nbsp;Highlights.js because:</p>\r\n<ol>\r\n<li>Support R lenguage.</li>\r\n<li>It's easy to use. Works automatically. That's mean you don't have to put the code what are you writing. Highloght it does alone.</li>\r\n<li>A lot of styles.</li>\r\n<li>And yep! Again! Support R lenguage.</li>\r\n</ol>\r\n<pre><code>\r\nlibrary(ggplot2)\r\ncentre &lt;- function(x, type, ...) {\r\n  switch(type,\r\n         mean = mean(x),\r\n         median = median(x),\r\n         trimmed = mean(x, trim = .1))\r\n}\r\n\r\nmyVar1\r\nmyVar.2\r\ndata$x\r\nfoo \"bar\" baz\r\n# test \"test\"\r\n\"test # test\"\r\n</code>\r\n</pre>\r\n<p>And an example of python code:</p>\r\n<pre><code>\r\ndef cov(x,y):\r\n    n = len(x)\r\n    xmn = mean(x)\r\n    ymn = mean(y)\r\n    xdeviations = [0]*len(x)\r\n    ydeviations = [0]*len(y)\r\n    for i in range(len(x)):\r\n        xdeviations[i] = x[i] - xmn\r\n        ydeviations[i] = y[i] - ymn\r\n    ss = 0.0\r\n    for i in range(len(xdeviations)):\r\n        ss = ss + xdeviations[i]*ydeviations[i]\r\n    return ss/float(n)\r\n\t\r\ndef cor(x,y):\r\n    return cov(x,y)/(var(x)*var(y))**.5\r\n</code></pre>\r\n<p>Nice!</p>", 
        "status": 1, 
        "allow_comments": false, 
        "title": "Testing highlight.js with r code", 
        "image": "images_pf/js_code.jpg", 
        "category": [
            3, 
            2
        ], 
        "pub_date": "2012-09-05T00:32:06Z", 
        "slug": "testing-highlightjs-r-code"
    }
},
{
    "pk": 4, 
    "model": "blog.entry", 
    "fields": {
        "body": "<p style=\"text-align: left;\">In an image each pixel is a color and that color hava a <a href=\"http://en.wikipedia.org/wiki/RGB_color_model\" target=\"_blank\">RGB representation</a>, this means each color is represented by a triplet. For example the red in this representation is (250, 0, 0), black is (0,0,0) and white is (250, 250, 250) (see <a href=\"http://cloford.com/resources/colours/500col.htm\" target=\"_blank\">here</a>&nbsp;more colors) . More generaly, each color is a point in the 3D cube [(0, 0, 0);(250, 250, 250)]. We'll transform an image to a data.frame with 3 columns where each observation it's a pixel. For make this we need to <a href=\"http://cran.r-project.org/web/packages/ReadImages/\" target=\"_blank\">ReadImages</a>&nbsp;and <a href=\"http://cran.r-project.org/web/packages/rgl\" target=\"_blank\">rgl</a> libraries. Let's take a look of the fist part of the script:</p>\r\n<pre><code class=\"r\">rm(list=ls())\r\nlibrary(ReadImages)\r\nlibrary(rgl)\r\n\r\n# Read the image\r\nimage &lt;- read.jpeg(\"images/nyiragongo-volcano-expedition-peter_52775_990x742.jpg\")\r\nstr(image)\r\nplot(image)\r\n\r\n# Obtaing the size of image\r\nH &lt;- dim(image)[1]\r\nW &lt;- dim(image)[2]\r\n\r\n# Creating the data frame\r\nrgb_image &lt;- data.frame(r = as.vector(image[1:H, 1:W, 1]),\r\n                        g = as.vector(image[1:H, 1:W, 2]),\r\n                        b = as.vector(image[1:H, 1:W, 3]))\r\n\r\n# I prefer work whit the rgb transformation\r\nrgb_image &lt;- round(rgb_image*250)\r\nhead(rgb_image)\r\n</code></pre>\r\n<p style=\"text-align: left;\">And we obtain the plot of te original image.</p>\r\n<p><img src=\"../../../../media/images/nyiragongo-volcano-expedition.jpg\" border=\"0\" width=\"300\" /></p>\r\n<p>Now for each obervation we'll obtain the rgb representation. Then we take a sample and plot those points with the respective color and this is the result.</p>\r\n<pre><code class=\"r\">rgb_image$colors_hex &lt;- rgb(rgb_image, max = 255)\r\nrgb_image_sample &lt;- rgb_image[sample(1:nrow(rgb_image), size = 6000),]\r\n\r\nwith(rgb_image_sample,{\r\n  plot3d(r, g, b, col = colors_hex, type='s', size=1, main = \"Original Colors\",\r\n         xlab = \"Red\", ylab = \"Green\", zlab = \"Blue\")\r\n})\r\nmovie3d(spin3d(axis=c(0,0,1)), duration=7, fps=10, movie = \"colors\", type = \"gif\")\r\n</code></pre>\r\n<p><img src=\"../../../../media/images/colors.gif\" border=\"0\" /></p>\r\n<p>There are black, blue and red points, and there are some yellow (red + green) points.&nbsp;Now we proceed to apply the k-mean algorithm over this points and plot the result.</p>\r\n<pre><code class=\"r\">\r\nkms &lt;- kmeans(rgb_image_sample[,1:3], centers=3)\r\nrgb_image_sample$color_kmeans &lt;- rgb(kms$centers[kms$cluster,], max = 255)\r\n\r\n\r\nwith(rgb_image_sample,{\r\n  plot3d(r, g, b, col = color_kmeans, type='s', size=1, main = \"Color Clusters\",\r\n         xlab = \"Red\", ylab = \"Green\", zlab = \"Blue\", scale = 0.2)\r\n})\r\nmovie3d(spin3d(axis=c(0,0,1)), duration=5, fps=40, movie = \"colors_kmeans\", type = \"gif\")\r\n</code></pre>\r\n<p><img src=\"../../../../media/images/colors_kmeans.gif\" border=\"0\" /></p>\r\n<p>&nbsp;</p>\r\n<p>This is for illustrate the problem but now we apply for the entire image and see the result.</p>\r\n<pre><code class=\"r\">kms &lt;- kmeans(rgb_image[,1:3], centers=3)\r\nrgb_new &lt;- kms$centers[kms$cluster, ]\r\n# Back to the original representation (intensity between (0,1))\r\nrgb_new &lt;- rgb_new/250\r\nimage_new &lt;- image\r\n\r\nimage_new[1:H, 1:W, 1] &lt;- rgb_new[,1]\r\nimage_new[1:H, 1:W, 2] &lt;- rgb_new[,2]\r\nimage_new[1:H, 1:W, 3] &lt;- rgb_new[,3]\r\n\r\nplot(image_new)</code></pre>\r\n<p>Finally we can loop over the k parameter and explore the evolution of the image.</p>\r\n<p><img src=\"../../../../media/images/nyiragongo-volcano-expedition.gif\" border=\"0\" /></p>\r\n<p>Maybe this is not a very useful application but it's a very good way to understand the idea of the k-means algorithm.</p>", 
        "status": 1, 
        "allow_comments": false, 
        "title": "K-means in images and R", 
        "image": "images_pf/kmeans_port.gif", 
        "category": [
            4, 
            2
        ], 
        "pub_date": "2012-09-06T01:29:53Z", 
        "slug": "k-means-images-and-r"
    }
},
{
    "pk": 5, 
    "model": "blog.entry", 
    "fields": {
        "body": "<p>I saw this <a href=\"http://uce.uniovi.es/mundor/howtoplotashapemap.html\" target=\"_blank\">post</a> and I decided to replicated that good example but with data closer to me, particulary data of my country. So, I've got the shape data of the capital of my country (You can download the data from <a href=\"../../../../media/data/SCL_Shape.zip\" target=\"_blank\">here</a>). The data comes from the 2002 CENSO&hellip; some old but very useful for this propose. The objective of this post is learn how to read, treat and plot this type of data.</p>\r\n<p>The packages what we'll need are:</p>\r\n<pre><code class=\"r\">library(maptools)\r\nlibrary(ggplot2)\r\nlibrary(plyr)\r\n</code></pre>\r\n<p>We need the data! Note that some shape file have data.</p>\r\n<pre><code class=\"r\">download.file('http://jkunst.com/media/data/SCL_Shape.zip',\r\n'SCL_Shape.zip') unzip('SCL_Shape.zip')\r\nmap &lt;- readShapeLines(\"RM_Comunas\")\r\nmapdata &lt;- map@data\r\n</code></pre>\r\n<p>Now we prepare the data obtaing the longitudes and latitudes from the data. We use the <em>fortify</em> (ggplot's) function. The result after apply this funtcion we don't now what group corresponds to a specific area (in this example a 'comuna'). Then we obtain the centers of the areas to put the names in a plot (for example).</p>\r\n<pre><code class=\"r\">cordsdata &lt;- fortify(map)\r\ncordsdata &lt;- join(cordsdata,\r\n    data.frame(group = unique(cordsdata$group),\r\n               COMUNA = unique(mapdata$COMUNA)),\r\n    type = \"left\", by = \"group\")\r\n\r\nmapdata &lt;- join(mapdata,\r\n                   ddply(cordsdata, .(COMUNA),\r\n                   summarize, CENTER.LONG = mean(long),\r\n                   CENTER.LAT = mean(lat)))\r\n</code></pre>\r\n<p>I this post we study only the <em>comunas</em> in the Santiago <em>province</em>. So, we select this cases. And plot the comunas for a first view to the data.</p>\r\n<pre><code class=\"r\">data_cords &lt;- subset(join(cordsdata, mapdata, type = \"left\"), PROVINCIA == \"SANTIAGO\")\r\ndata_map &lt;- subset(mapdata, PROVINCIA == \"SANTIAGO\")\r\n\r\ng &lt;- ggplot(data_cords) + theme(legend.position = \"none\", axis.ticks = element_blank(), \r\n    axis.text = element_blank(), axis.title = element_blank())\r\n\r\n# Showing the comunas of Santiago\r\ng + geom_polygon(aes(x = long, y = lat, group = group), color = \"gray\", alpha = 0.7) + \r\n    geom_text(aes(x = CENTER.LONG, y = CENTER.LAT, label = COMUNA), size = 2, \r\n        color = \"white\") + ggtitle(\"The Comunas in Santiago\")\r\n</code></pre>\r\n<p><img src=\"../../../../media/images/plotmap1.png\" border=\"0\" /></p>\r\n<p>Now we plot the previous map but showing the comunas with more density (population/area). The firts of this comunas are <em>Lo Prado</em> o <em>San Ram&oacute;n</em>, this comunas are places were exists a lot of suburbs.</p>\r\n<pre><code class=\"r\">head(as.character(data_map[order(data_map$DENSIDAD, decreasing = T), ]$COMUNA), \r\n    5)\r\n</code></pre>\r\n<pre><code>## [1] \"LO PRADO\"            \"SAN RAMON\"           \"CERRO NAVIA\"        \r\n## [4] \"LO ESPEJO\"           \"PEDRO AGUIRRE CERDA\"\r\n</code></pre>\r\n<pre><code class=\"r\">g + geom_polygon(aes(x = long, y = lat, group = group, alpha = POBLACION/AREA), \r\n    fill = \"darkred\", color = \"white\") + ggtitle(\"Density of population in each Comuna\")\r\n</code></pre>\r\n<p><img src=\"../../../../media/images/plotmap2.png\" border=\"0\" /></p>\r\n<p>If we plot now the comunas according the numbers of persons. In this case the comunas with more people are <em>Maip&uacute;</em> and <em>La Florida</em>, in my country this comunas have the nickname <em>Bedroom Comunas (Comunas dormitorio)</em> becuase this comunas have a large area and them have a lot of housing. So, the most of people in this areas don't work there, instead they have their housgins.</p>\r\n<pre><code class=\"r\">head(as.character(data_map[order(data_map$POBLACION, decreasing = T), ]$COMUNA, \r\n    5))\r\n</code></pre>\r\n<pre><code>## [1] \"MAIPU\"      \"LA FLORIDA\" \"LAS CONDES\" \"PENALOLEN\"  \"SANTIAGO\"  \r\n## [6] \"PUDAHUEL\"\r\n</code></pre>\r\n<pre><code class=\"r\">g + geom_polygon(aes(x = long, y = lat, group = group, alpha = POBLACION), fill = \"blue\", \r\n    color = \"white\") + ggtitle(\"Population in each Comuna\")\r\n</code></pre>\r\n<p><img src=\"../../../../media/images/plotmap3.png\" border=\"0\" alt=\"plot of chunk plotmap3\" /></p>\r\n<p>Well, this is a first approach to analysis this type of data. I hope make other interesting things soon.</p>", 
        "status": 1, 
        "allow_comments": false, 
        "title": "A brief script on Geographical data analysis in R", 
        "image": "images_pf/scl_comunas_pf.png", 
        "category": [
            2
        ], 
        "pub_date": "2012-10-10T03:30:19Z", 
        "slug": "brief-script-geographical-data-analysis-r"
    }
},
{
    "pk": 6, 
    "model": "blog.entry", 
    "fields": {
        "body": "<p>In this post we'll fit some predicitve models in (well know) data bases, and evalute the performance of each model. <strong>Disclaimer1</strong>: for simplicity the predictive variables are treating without apply any transformation to get a better performance or stability, etc. We'll use two datas to evaluate the performances of the models. Both data have categorical and continous variables and we'll use 50-50 split to have a train and test data. The datas are:</p>\r\n<ul>\r\n<li><strong>German Credit</strong>: The German Credit data frame has 1000 rows and 8 columns. This are data for clients of a south german bank, 700 good payers and 300 bad payers. They are used to construct a credit scoring method. This data have 20 predictive variables and 1000 observations and have a bad rate of 30%. So, after the select subsets to fit the models the distributions in the data are:</li>\r\n</ul>\r\n<pre><code>##         sample\r\n## response test  train\r\n##     bad  14.9% 15.1%\r\n##     good 38.1% 31.9%\r\n</code></pre>\r\n<ul>\r\n<li><strong>Bankloan Binning</strong>: This is a hypothetical data file containing financial and demographic information on past customers. Here is the <a href=\"http://publib.boulder.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fdata_files.htm\" target=\"_blank\">source</a>. This data have 8 predictive variables and 5000 observations and have a bad rate of 25.1%.</li>\r\n</ul>\r\n<pre><code>##         sample\r\n## response test  train\r\n##     bad  12.7% 12.4%\r\n##     good 37.3% 37.6%\r\n</code></pre>\r\n<p>The models to compare are logistic, conditional inference trees (<a href=\"http://cran.r-project.org/web/packages/party/index.html\" target=\"_blank\">party</a> package), single-hidden-layer neural network (<a href=\"http://cran.r-project.org/web/packages/nnet/index.html\" target=\"_blank\">nnet</a> pakage) and linear discriminant analysis. To evalute the performance there are some indicators like KS statistic, Area under ROC curve among others. If you are not familiar with this terms check this <a href=\"http://www.saedsayad.com/model_evaluation_c.htm\" target=\"_blank\">link</a>. Now, let's go with the results.</p>\r\n<h4>German Credit Data</h4>\r\n<pre><code>##            SCORE SAMPLE   BR   KS AUCROC Gain10 Gain20 Gain30 Gain40 Gain50\r\n## 2 SCORE_Logistic  train 0.32 0.54   0.83   0.25   0.48   0.64   0.74   0.82\r\n## 4    SCORE_CTree  train 0.32 0.40   0.76   0.53   0.53   0.61   0.87   0.87\r\n## 6   SCORE_SLNNET  train 0.32 0.64   0.90   0.30   0.54   0.73   0.82   0.89\r\n## 8      SCORE_LDA  train 0.32 0.54   0.83   0.24   0.47   0.63   0.76   0.82\r\n## 1 SCORE_Logistic   test 0.28 0.46   0.78   0.23   0.43   0.58   0.72   0.82\r\n## 3    SCORE_CTree   test 0.28 0.37   0.74   0.50   0.50   0.57   0.87   0.87\r\n## 5   SCORE_SLNNET   test 0.28 0.42   0.77   0.21   0.40   0.58   0.68   0.79\r\n## 7      SCORE_LDA   test 0.28 0.47   0.79   0.23   0.46   0.57   0.73   0.82\r\n</code></pre>\r\n<pre><code class=\"r\">daux &lt;- subset(data1, SAMPLE == \"test\")\r\ndaux_roc &lt;- ldply(str_pattern(names(daux),\"SCORE\"), function(score){\r\n  perf &lt;- performance(prediction(daux[[score]], daux$GB), \"tpr\",\"fpr\")\r\n  df &lt;- data.frame(x = unlist(perf@\"x.values\") , y = unlist(perf@\"y.values\"))\r\n  df$score &lt;- score\r\n  df\r\n})\r\n\r\nggplot(daux_roc) + geom_line(aes(x,y, color = score), size = 1.2) + \r\n  scale_color_manual('',values=brewer.pal(length(unique(daux_roc$score)), \"RdBu\")) +\r\n  geom_path(data=data.frame(x = c(0,1), y = c(0,1)),\r\n            aes(x,y), colour = \"gray\", size = 1) +\r\n  scale_x_continuous(\"False Positive Rate (1 - Specificity)\",\r\n                     labels = percent_format(), limits = c(0, 1)) +\r\n  scale_y_continuous(\"True Positive Rate (Sensivity or Recall)\",\r\n                     labels = percent_format(), limits = c(0, 1)) +\r\n  theme(legend.position = \"top\") +\r\n  ggtitle(\"ROC Curves for German Credit Data (validation)\")\r\n</code></pre>\r\n<p><img src=\"../../../../media/images/distributions-1.png\" border=\"0\" alt=\"plot of chunk roc-curve-1\" /></p>\r\n<p>Now we can plot the distributions of good/bads in each model. We'll transform the data whith melt function and then plot faceting by score.</p>\r\n<pre><code class=\"r\">daux &lt;- subset(data1, SAMPLE == \"test\",\r\n               select = c(\"GB\", \"SCORE_Logistic\",\r\n                          \"SCORE_CTree\",\"SCORE_SLNNET\",\"SCORE_LDA\"))\r\ndaux &lt;- melt(daux, id = \"GB\")\r\n\r\nggplot(daux, aes(x=value, fill = factor(GB))) +\r\n  geom_density(alpha = 0.6, size = .75) +\r\n  facet_wrap(~variable, ncol=2) +\r\n  scale_fill_manual(values = brewer.pal(3, \"Dark2\")) +\r\n  theme(legend.position = \"none\",\r\n        axis.ticks = element_blank(),\r\n        axis.text = element_blank(),\r\n        axis.title = element_blank(),\r\n        plot.margin = unit(rep(0.5, 4), \"lines\"),\r\n        title = element_text(size = 9))\r\n</code></pre>\r\n<p><img src=\"../../../../media/images/roc-curve-1.png\" border=\"0\" alt=\"plot of chunk distributions-1\" /></p>\r\n<h4>Bankloan Binning data</h4>\r\n<pre><code>##            SCORE SAMPLE   BR   KS AUCROC Gain10 Gain20 Gain30 Gain40 Gain50\r\n## 2 SCORE_Logistic  train 0.25 0.54   0.84   0.31   0.51   0.67   0.79   0.89\r\n## 4    SCORE_CTree  train 0.25 0.52   0.84   0.34   0.50   0.82   0.82   0.90\r\n## 6   SCORE_SLNNET  train 0.25 0.56   0.86   0.32   0.52   0.68   0.82   0.89\r\n## 8      SCORE_LDA  train 0.25 0.52   0.84   0.31   0.51   0.66   0.78   0.88\r\n## 1 SCORE_Logistic   test 0.25 0.53   0.84   0.31   0.51   0.68   0.78   0.87\r\n## 3    SCORE_CTree   test 0.25 0.45   0.79   0.30   0.51   0.76   0.76   0.84\r\n## 5   SCORE_SLNNET   test 0.25 0.51   0.83   0.30   0.51   0.66   0.77   0.85\r\n## 7      SCORE_LDA   test 0.25 0.50   0.83   0.31   0.51   0.67   0.77   0.85\r\n</code></pre>\r\n<p><img src=\"../../../../media/images/distributions-2.png\" border=\"0\" alt=\"plot of chunk roc-curve-2\" /></p>\r\n<p><img src=\"../../../../media/images/roc-curve-2.png\" border=\"0\" alt=\"plot of chunk distributions-2\" /></p>\r\n<p>Do you want to comment about the results? If you are interesting in this topic reproduce this example. And if you have questions and/or improvements or want to know more details for the code please comment.</p>\r\n<h4>References</h4>\r\n<ol>\r\n<li><a href=\"http://www.ggplot2.org\" target=\"_blank\">Ggplot2</a></li>\r\n<li><a href=\"http://www.rstudio.com/ide/\" target=\"_blank\">RStudio</a></li>\r\n<li><a href=\"http://yihui.name/knitr/\" target=\"_blank\">Knitr</a></li>\r\n<li><a href=\"http://cran.r-project.org/doc/contrib/Sharma-CreditScoring.pdf\" target=\"_blank\">Guide to credit scoring in R</a></li>\r\n</ol>", 
        "status": 1, 
        "allow_comments": false, 
        "title": "Credit Scoring in R 101", 
        "image": "images_pf/benchmark_scores_pf.png", 
        "category": [
            2, 
            4
        ], 
        "pub_date": "2012-11-07T03:30:43Z", 
        "slug": "credit-scoring-r-101"
    }
},
{
    "pk": 7, 
    "model": "blog.entry", 
    "fields": {
        "body": "<p>The las week I knew the r package&nbsp;<a href=\"http://cran.r-project.org/web/packages/rgexf/\" target=\"_blank\">rgexf&nbsp;</a>made by <a href=\"http://www.nodoschile.org/\" target=\"_blank\">George Vega Yon</a>.&nbsp;Rgexf is a R library to work with GEXF graph \ufb01les. This type of files allow represent networks in a xml. So, if you have a list of nodes and a data frame of edges (source-target) you can obtain a gexf file with <strong>write.gexf </strong>function. Then you have a gexf file which can open with <a href=\"https://gephi.org/\" target=\"_blank\">Gephi</a> or use with <a href=\"http://sigmajs.org/\" target=\"_blank\">Sigma.js</a> to show via web. Simple, right?</p>\r\n<p>Now, if you work with your data and want to visualize your gexf object you must open Gephi or create a <a href=\"http://docs.python.org/2/library/simplehttpserver.html\" target=\"_blank\">simple http server</a> to view your chart using SigmaJS (this is because for security a reason, I guess, like <a href=\"http://stackoverflow.com/questions/8009825/cannot-view-gvismotionchart-from-printed-html-file\" target=\"_blank\">this</a>). For not do this extra step we can use the <a href=\"http://cran.r-project.org/web/packages/Rook/index.html\" target=\"_blank\">Rook </a>&nbsp;R&nbsp;library for print gexf objects more quickly and easy.</p>\r\n<p>Basically we need to make a RHttp element with 2 apps. One app for show the plot (using SigmaJS) and another app to access the data made by write.gefx. We need a template for made the first app, I used the example founded <a href=\"https://forum.gephi.org/viewtopic.php?t=1900&amp;f=28\" target=\"_blank\">here</a> and I modified a little (<a href=\"../../../../media/rgexf_sigma/index.html\" target=\"_blank\">here</a>, \"save link as\"), this index.html must be in your working directory. Now the function is:</p>\r\n<pre><code class=\"r\"> plot.gexf &lt;- function(gexf.object){\r\n      library(Rook)\r\n      graph &lt;- gexf.object$graph\r\n      s &lt;- Rhttpd$new()\r\n      s$start(listen='127.0.0.1')\r\n      my.app &lt;- function(env){\r\n            res &lt;- Response$new()\r\n            res$write(paste(readLines(\"index.html\", warn=F), collapse=\"\\n\"))\r\n            res$finish()\r\n      }\r\n\r\n      s$add(app=my.app, name='plot')\r\n\r\n      my.app2 &lt;- function(env){\r\n            res &lt;- Response$new()\r\n            res$write(graph)\r\n            res$finish()\r\n      }\r\n\r\n      s$add(app=my.app2, name='data')\r\n      s$browse('plot')&nbsp;\r\n}</code></pre>\r\n<p>Now we need to create the gexf object.</p>\r\n<pre><code class=\"r\">\r\nnNodes &lt;- 100\r\nnRelations &lt;- 200\r\n\r\nnodes &lt;- data.frame(id = c(1:nNodes),\r\n                    names = c(1:nNodes))\r\n\r\n\r\nallrelations &lt;- as.data.frame(t(combn(nNodes, 2)))\r\nrelations &lt;- allrelations[sample(1:nrow(allrelations),\r\n                                 size = min(c(nRelations, nrow(allrelations)))),]\r\nnames(relations) &lt;- c(\"target\", \"source\")\r\n\r\nnodecolors &lt;- data.frame(r = sample(1:249, size = nrow(nodes), replace=T),\r\n                         g = sample(1:249, size = nrow(nodes), replace=T),\r\n                         b = sample(1:249, size = nrow(nodes), replace=T),\r\n                         a = runif(nrow(nodes), min=.5, max=1))\r\n\r\n\r\nnodesizes &lt;- sample(50:500, size=nrow(nodes), replace=T)\r\nedgethicks &lt;- sample(50:500, size=nrow(relations), replace=T)\r\n</code>\r\n</pre>\r\n<p>This is all random, the color, sizes, etc. But if the position of each node is random the visualization will not achive its purpouse: find agglomerations or find groups of nodes more closely between them. For this reason we can use the <a href=\"http://cran.r-project.org/web/packages/sna/\" target=\"_blank\">sna</a>&nbsp;package to find an optimal layout to show the nodes depending the links between them (if you know other packages with more algorithms please email me!).</p>\r\n<pre><code class=\"r\">links &lt;- matrix(rep(0, nNodes*nNodes), ncol = nNodes)\r\nfor(edge in 1:nRelations){\r\n      links[(relations[edge,]$target), (relations[edge,]$source)] &lt;- 1\r\n}\r\n\r\nlibrary(sna)\r\n\r\npositions &lt;- gplot.layout.mds(links, layout.par=list())\r\n\r\npositions &lt;- cbind(positions, 0) # needs a z axis\r\n</code>\r\n</pre>\r\n<p>Finally we create the graph with the parameters and we plot it!</p>\r\n<pre><code class=\"r\">graph &lt;- write.gexf(nodes=nodes,\r\n                    edges=relations,\r\n                    nodesVizAtt=list(\r\n                      color=nodecolors,\r\n                      size=nodesizes,\r\n                      position=positions\r\n                    ),\r\n                    edgesVizAtt=list(\r\n                      thickness= edgethicks\r\n                    ))\r\n                    \r\nplot.gexf(graph)\r\n</code>\r\n</pre>\r\n<p>And you'll obtain something like <a href=\"../../../../media/rgexf_sigma/index_sigmajs_example.html\" target=\"_blank\">this live example</a>. Have fun ;)!</p>\r\n<p><img src=\"../../../../media/images/plot.gefx.png\" border=\"0\" alt=\"plot.gefx.example\" /></p>\r\n<p><strong>Update:</strong></p>\r\n<p>The are many algorithms to find layout of the network in the sna package</p>\r\n<pre><code class=\"r\"># positions &lt;- gplot.layout.adj(links, layout.par=list())\r\n# positions &lt;- gplot.layout.circle(links, layout.par=list())\r\n# positions &lt;- gplot.layout.circrand(links, layout.par=list())\r\n# positions &lt;- gplot.layout.eigen(links, layout.par=list())\r\n# positions &lt;- gplot.layout.fruchtermanreingold(links, layout.par=list())\r\n# positions &lt;- gplot.layout.geodist(links, layout.par=list())\r\n# positions &lt;- gplot.layout.hall(links, layout.par=list())\r\n# positions &lt;- gplot.layout.kamadakawai(links, layout.par=list())\r\npositions &lt;- gplot.layout.mds(links, layout.par=list())\r\n# positions &lt;- gplot.layout.princoord(links, layout.par=list())\r\n# positions &lt;- gplot.layout.random(links, layout.par=list())\r\n# positions &lt;- gplot.layout.rmds(links, layout.par=list())\r\n# positions &lt;- gplot.layout.segeo(links, layout.par=list())\r\n# positions &lt;- gplot.layout.seham(links, layout.par=list())\r\n# positions &lt;- gplot.layout.spring(links, layout.par=list())\r\n# positions &lt;- gplot.layout.springrepulse(links, layout.par=list())\r\n# positions &lt;- gplot.layout.target(links, layout.par=list())</code></pre>", 
        "status": 1, 
        "allow_comments": false, 
        "title": "Having fun with rgefx package and sigmajs in R", 
        "image": "images_pf/rgexf_port.png", 
        "category": [
            2, 
            3
        ], 
        "pub_date": "2013-02-13T02:12:52Z", 
        "slug": "having-fun-rgefx-package-and-sigmajs-r"
    }
},
{
    "pk": 8, 
    "model": "blog.entry", 
    "fields": {
        "body": "<p>The last post I show a way to&nbsp;plot a gexf file in R using the <a href=\"http://cran.r-project.org/web/packages/rgexf/\" target=\"_blank\">rgexf</a> package and the <a href=\"http://sigmajs.org/\" target=\"_blank\">Sigmajs</a> library. Now we need some data to use that piece of code. So I've decided obtain the tweets about R. For this I've used the <a href=\"http://cran.r-project.org/web/packages/twitteR/index.html\" target=\"_blank\">twitteR</a> package and search \"#rstats\", then clean the texts and extract all the hashtags. Then find the associations between following the next simple rule: if a tweet said: \"<strong>#rstats and #data are my drugs</strong>\" this two hashtags are related. Then put some graphics attributes like size of the node according the quantity of mentions, and some random to make the graph more attractive.</p>\r\n<p>Finally make te code run and see the&nbsp;<a href=\"../../../../media/rgexf_sigma/index_sigmajs_what_it_say.html\" target=\"_blank\">result ;)</a>.</p>\r\n<p>There are many tweets about #python in the #rstats 's tweets!. It is obvious see many tweets about data (#data, #bi, #datamining, #bigdata, etc). In other hand there are conversations about #sas, #matlab, and #sastip, and so on.</p>\r\n<pre><code class=\"r\">\r\nlibrary(twitteR)\r\nlibrary(stringr)\r\nlibrary(plyr)\r\nlibrary(sna)\r\n\r\n\r\n# Some tweets about R\r\ntweets &lt;- tolower(twListToDF(searchTwitter(searchString=\"#rstats\", n=1500))$text)\r\nhead(tweets)\r\nhashtags_remove &lt;- c(\"#rstats\", \"#r\")\r\n\r\n# Cleaning the tweets\r\nfor(term in hashtags_remove) tweets &lt;- gsub(term, \"\", tweets)\r\n\r\n# Extract the hastags\r\nhashtags &lt;- unique(unlist(str_extract_all(tolower(tweets), \"#\\\\w+\")))\r\nhashtags &lt;- setdiff(hashtags, hashtags_remove)\r\n\r\n# Capture the node size according the amount that appear\r\nnodesizes &lt;- laply(hashtags, function(hashtag){\r\n  sum(grepl(hashtag, tweets))\r\n})\r\n\r\n\r\n# scaling  sizes\r\nnodesizes &lt;-  1 + log(nodesizes, base = 3)\r\n\r\n\r\nnodes &lt;- data.frame(id = c(1:length(hashtags)), label = hashtags, stringsAsFactors=F)\r\n\r\n#\r\nrelations &lt;- ldply(hashtags, function(hashtag){\r\n  hashtag_related &lt;- unlist(str_extract_all(tweets[grepl(hashtag, tweets)], \"#\\\\w+\"))\r\n  hashtag_related &lt;- setdiff(hashtag_related, hashtag) \r\n  if(length(hashtag_related)==0){\r\n    return(data.frame())\r\n  }\r\n  data.frame(source = which(hashtags==hashtag),\r\n             target =  which(hashtags %in% hashtag_related))\r\n})\r\n\r\n# Is an undirected graph! So remove the duplicates\r\nfor(row in 1:nrow(relations)){  \r\n  relations[row,] &lt;- sort(relations[row,])\r\n}\r\n\r\nrelations &lt;- unique(relations)\r\n\r\n\r\n# Some colors\r\nnodecolors &lt;- data.frame(r = sample(1:249, size = nrow(nodes), replace=T),\r\n                         g = sample(1:249, size = nrow(nodes), replace=T),\r\n                         b = sample(1:249, size = nrow(nodes), replace=T),\r\n                         a = 1)\r\n\r\n\r\nlinks &lt;- matrix(rep(0, length(hashtags)^2), ncol = length(hashtags))\r\nfor(edge in 1:nrow(relations)){\r\n      links[(relations[edge,]$target), (relations[edge,]$source)] &lt;- 1\r\n}\r\n\r\npositions &lt;- gplot.layout.kamadakawai(links, layout.par=list())\r\npositions &lt;- cbind(positions, 0) # needs a z axis\r\n\r\n\r\n\r\ngraph &lt;- write.gexf(nodes=nodes,\r\n                    edges=relations,\r\n                    nodesVizAtt=list(\r\n                      color=nodecolors,\r\n                      size=nodesizes,\r\n                      position=positions))\r\n\r\nplot.gexf(graph)\r\n</code>\r\n</pre>", 
        "status": 1, 
        "allow_comments": false, 
        "title": "What does it say about r?", 
        "image": "images_pf/what_it_say_about_r.png", 
        "category": [
            4, 
            2, 
            5, 
            3
        ], 
        "pub_date": "2013-02-18T21:30:42Z", 
        "slug": "what-does-it-say-about-r"
    }
},
{
    "pk": 9, 
    "model": "blog.entry", 
    "fields": {
        "body": "<p>I took the <a href=\"https://www.coursera.org/course/sna\" target=\"_blank\">SNA course</a> by <a href=\"http://www.ladamic.com/\" target=\"_blank\">Lada Adamic</a> in coursera. It's a super interesting course. In fact, I was using the networks only how a visualization tool, and that is what it make me little bit embarrassing because there are more, a lot of more. You can detect communities, know the more centric nodes and a lot of other information. So, there are a lot of reasons to look the course.</p>\r\n<p>By other hand I like the d3 javascript library. Recently I was learning javascript, so I decided make a very little app to keep learning this library and show differents measures of centrality for each node in a set of 4 toy networks and see these measures by size, color or a label.&nbsp;</p>\r\n<p>In this <a href=\"https://github.com/jbkunst/R-D3-SNA-Course-Example\" target=\"_blank\">respositoy</a> you can find the R code which create the networks, and export the data in json format.</p>\r\n<p>&nbsp;</p>\r\n<p><iframe src=\"../../../../media/sandbox/SNA/index.html\" width=\"590px\" height=\"600px\"></iframe></p>", 
        "status": 1, 
        "allow_comments": false, 
        "title": "R, D3.js and SNA Course", 
        "image": "images_pf/nets.000.png", 
        "category": [
            5, 
            3, 
            2, 
            6, 
            4
        ], 
        "pub_date": "2013-04-05T08:18:11Z", 
        "slug": "r-d3js-and-sna-course"
    }
}
]
